\chapter{Stand der Technik}
\begin{onehalfspace}  
    \label{sec:theorie/standdertechnik}
        In diesem Kapitel wird der Stand der Technik näher beleuchtet. Der Fokus liegt dabei auf den Themen Daten, \ac{KI} und Bias. Zu Beginn wird auf basis der Literatur erläutert, was Daten sind, was Datenqualität bedeutet und worum es sich bei einem Bias handelt. Daraufhin wird näher auf \ac{KI}, das Teilgebiet \ac{ML} und die Ethik in der \ac{KI} eingegangen. Zuletzt werden die Themen in einen gemeinsamen Kontext gebracht und der Einfluss eines Bias auf eine \ac{KI} betrachtet sowie Gegenmaßnahmen untersucht. 
    
    \section{Daten als wertschöpfende Ressource}
    \label{subsec:datenchapter}
    \subsection{Daten}
    \label{subsubsec:daten}
        Dass Daten eine wertvolle Ressource seien, meinte bereits 2006 der britische Mathematiker Clive Humby mit dem berühmten Zitat: \glqq{}Data is the new oil\grqq{}.\cite{Frorbes2021} Hiermit ist gemeint, dass Daten in ihre Rohform nicht sonderlich wertvoll sind, diese jedoch an Wert gewinnen, sobald man beginnt sie zu verarbeiten. Denn lange Zeit waren Daten nur ein Nebenprodukt der Digitalisierung. Daten wurde gesammelt und gespeichert, aber nicht weiter verwendet. Mit dem technologischen Fortschritt im Bereich von Datenanalysen und mit aufkommen der \ac{KI} wurden Daten von Zeit zu Zeit immer wertvoller. So wurden neue Datengetriebene Geschäftsfeld ermöglicht, die einen Mehrwert aus Rohdaten schaffen können. Insbesondere das rasante Aufkommen des Internet of Things hat diese Entwicklung stark vorangetrieben. Seither steigt die Menge der jährlich gesammelten Daten exponentiell an.\cite{Otto2019}
        \\
        \begin{figure}[h]
            \centering
            \includegraphics[width = 15.5cm]{Bilder/Annual_Data_Size.png}
            \caption{Weltweit jährlich anfallende Datenmenge \cite{Reinsel2018}}
            \label{fig:DataSize}
        \end{figure}
        \\
        Abbildung \ref{fig:DataSize} stammt aus dem Jahr 2018 und verdeutlicht, dass bereits damals erwartet wurde, dass bis im Jahr 2025 rund 175 Zetabyte Daten jährlich gesammelt werden. Im Vergleich dazu waren es 2018 gerade einmal 33 Zetabyte weltweit.\cite{Reinsel2018}\cite{Taleb2018}
        \\
        Der Begriff Daten selbst wird im ISO2382-1 Standard wie folgt definiert: \glqq{}reinterpretierbare Darstellung von Informationen in einer formalisierten Weise, die für die Kommunikation, Interpretation oder Verarbeitung geeignet ist\grqq{}.\cite{ISO2382} Daraus lässt sich ableite, dass Daten Informationen der Vergangenheit repräsentieren und für zukünftige Verwendung die Informationen aus der Vergangenheit in einer einheitlichen Form repräsentieren. Damit ist jedoch nicht die einheitliche Form der Daten selbst gemeint. 
        \\
        Daten gibt es in unterschiedlichen Formen. Es wird zwischen strukturierten und unstrukturierten Daten unterschieden. Strukturierte Daten sind Datensätze bestehend aus einzelnen Variablen die eindeutige Größen Darstellen. Beispiel hierfür sind Sensordaten oder Unternehmenszahlen aus einem ERP System. Sie werden tabellarisch gespeichert und können einfach weiter verarbeitet werden. Oftmals sind diese Daten heterogen, was bedeutet, dass sich die Variablen unterscheiden und bspw. Spalte 1 vollkommen andere Daten beinhaltet als Spalte 2. Ein Beispiel hierfür wären Sensoren für Luftfeuchtigkeit und Helligkeit in einem Büro. Als unstrukturierte Daten bezeichnet man Daten, die nicht in sinnvolle einheitliche Variablen unterteilt werden können. Zu dieser Art von Daten zählt man Bilder, Videos, Audio und Textdaten. Sie sind meist homogen, denn die Pixel in einem Bild nehmen zwar unterschiedliche RGB Werte an, jedoch repräsentieren sie alle einen Pixel. Dabei ist es egal ob es sich um Pixel 1 oder Pixel 42 handelt.\cite{Horn2022} Bei dieser unvorstellbar großen Datenmenge die jährlich generiert wird, wird davon ausgegangen, dass rund 80\% als unstrukturierte Daten vorliegen.\cite{Otto2019}
        \\
        Häufig wird in dem Zusammenhang mit Daten auch von Big Data gesprochen. Eine einheitliche Definition für den Begriff Big Data existiert jedoch nicht. Denn der Begriff Big Data umfasst die gesamte Wertschöpfungskette. Diese beinhaltet die Datenerzeugung, das Sammel und Speichern der Daten bis hin zur Verarbeitung und Nutzung für Analysen oder Visualisierungen.\cite{Taleb2018}\cite{Faroukhi2020} Es handelt sich dabei um Informationen mit hohem Volumen (high-volume), hoher Geschwindigkeit (high-velocity) und hoher Vielfalt(high-variety). Diese drei charakteristischen Eigenschaften werden in der Literatur auch als die \glqq{}3 V rule\grqq{} bezeichnet und ist in den meisten Definitionen wiederfinden.\cite{Taleb2018}\cite{Yalaoui2021} 
        \begin{figure}[h]
            \centering
            \includegraphics[width = 6.7cm]{Bilder/3VRule.png}
            \caption{Big Data 3 V rule \cite{Taleb2018}}
            \label{fig:3VRule}
        \end{figure}
        \\
        In der Abbildung \ref*{fig:3VRule} werden diese Eigenschaften die Big Data mit sich bringt näher beschrieben. Unter \glqq{}Volume\grqq{} wird die große Menge an Daten, also der damit verbundene benötigte Speicher, und ihre Skalierbarkeit betrachtet. Bei der \glqq{}Velocity\grqq{} liegt der Fokus auf der Geschwindigkeit in der die Daten erzeugt werden. Dies hat wiederum je nach Geschwindigkeit der Datenerzeugung Einfluss auf das Volumen. Abschließend in dem Dreieck gibt es die \glqq{}Variety\grqq{}. Big Data ist von Natur aus eine Vielfalt an strukturierten und unstrukturierten Daten. All diese drei Eigenschaften beeinflussen sich gegenseitig und bilden die grundlegenden Eigenschaften von Big Data.
        \\
        In Rohform sind diese Daten, wie eingangs erwähnt, jedoch nicht sonderlich von Wert. Egal ob Big Data oder nicht, einen Mehrwert und Informationen liefern sie erst, sobald man sie nutzt. Dabei ist es egal ob für Simulationen, Monitoring oder \ac{KI}. In der Vergangenheit sind Daten ein Nebenprodukt der Digitalisierung gewesen. Heute sind sie ein eigenes Geschäftsfeld und \glqq{}Enabler\grqq{} für viele bisher nicht möglich gewesenen Anwendungen.\cite{Otto2019}\cite{Gröger2021}

    \subsection{Datenqualität}
    \label{subsubsec:datenqualität}
        Im Zusammenhang mit Daten fällt immer häufiger auch der Begriff Datenqualität. Hier trifft Quantität auf Qualität. Wie bereits erwähnt, ist die Menge an Daten die bereits zur Verfügung steht, riesig. Quantität ist daher nicht das Problem. Die Qualität der Daten hat hier jedoch sehr großen Einfluss. Nicht selten können Daten nicht verwendet werden, da die Qualität nicht ausreichend ist. Gerade für Analysen, Auswertungen und Vorhersagen, wie sie durch die \ac*{KI} getroffen werden sollen, wird hohe Datenqualität benötigt.\cite{Byabazaire2020}
        \\
        Datenqualität selbst lässt sich auf unterschiedliche Arten und Weisen verstehen.\cite{Yalaoui2021} Eine gängige Definition für Datenqualität in der Literatur ist:\glqq{}fitness for use\grqq{}.\cite{Faroukhi2020} Es bedeutet, dass die Datenqualität von Nutzungskontext und Anwendungsfall abhängt und in erster Linie nicht allgemeine Qualitätsanforderungen erfüllt werden, sondern die für den Use Case benötigten Qualitätsanforderungen.\cite{Faroukhi2020}\cite{Yalaoui2021}
        \begin{figure}[h]
            \centering
            \includegraphics[width = 9cm]{Bilder/Data_quality_dimensions.png}
            \caption{Data Quality Dimensions \cite{Taleb2018}}
            \label{fig:DataQualityDimensions}
        \end{figure}
        \\
        Für die Validierung der Datenqualität gibt es die sogenannten Data Quality Dimensions. In Abbildung \ref{fig:DataQualityDimensions} sind die vier unterschiedlichen Dimensionen: Intrinsic (\ac{dt}.: Intrinsisch), Contextual (\ac{dt}.: Kontextbezogen), Accessibility (\ac{dt}.: Zugänglichkeit ) und Representational (\ac{dt}.: Repräsentativ) dargestellt. Jede Dimension besitzt eigene Merkmale an denen die Datenqualität gemessen werden kann. Näher erläutert werden die Dimensionen in der Tabelle \ref*{table:0}.
        \\
        \begin{table}[h]
            \centering
            \begin{adjustbox}{width=\textwidth}
            \resizebox{\textwidth}{!}{\begin{tabular}{|l|l|}
            \hline
            Intrinsic        & \begin{tabular}[c]{@{}l@{}}
                                    Zur intrinsischen Datenqualität gehören, wie in Abbildung \ref*{fig:DataQualityDimensions} zu sehen,\\
                                    die Merkmale: Zeitlos, Vollständigkeit, Genauigkeit, Konsistenz. Die \\
                                    intrinsische Dimension bildet die Qualitätsmerkmale der Daten selbst ab. \\
                                    und gibt Aufschluss über Objektivität und Glaubwürdigkeit der Daten.\cite{Yalaoui2021}
                                \end{tabular}  \\ \hline
            Contextual      &   \begin{tabular}[c]{@{}l@{}}
                                    Bei der kontextbezogenen Datenqualität wird betrachtet in welchem Ausmaß \\
                                    die Daten für den Nutzenden einsetzbar sind. Darin werden Eigenschaften wie:\\
                                    Datenmenge (Quantität), Zugänglichkeit, Aktualität und Mehrwert \\
                                    der Daten betrachtet um die Nutzbarkeit zu messen.\cite{Otto2019}
                                \end{tabular}  \\ \hline
            Representational &  \begin{tabular}[c]{@{}l@{}}
                                    Repräsentativ bedeutet im Kontext der Datenqualität, dass die Daten \\
                                    Interpretierbarkeit und dabei primär einfach zu Verstehen sind, \\
                                    Konsistent sind und  Manipulierbarkeit, also Veränderbar sind.\cite{Byabazaire2020}

                                \end{tabular}  \\ \hline
            Accessibility    &  \begin{tabular}[c]{@{}l@{}}
                                    Die vierte Dimension, die Zugänglichkeit, bezieht sich Insbesondere \\
                                    auf das Speichersystem der Daten. Diese Dimension wird an der möglichst\\
                                    einfachen Zugänglichkeit und der entgegenstehenden Sicherheit der \\
                                    Daten gemessen.\cite{Byabazaire2020}
                                \end{tabular}  \\ \hline           
            \end{tabular}}
            \end{adjustbox}
            \caption{Data Quality Dimensions Merkmale}
            \label{table:0}
        \end{table}
        \\
        Datenqualität lässt sich in unterschiedlichsten Dimensionen und anhand unterschiedlicher Qualitätsmerkmale messen. Die in Abbildung \ref*{fig:DataQualityDimensions} dargestellten und in Tabelle \ref*{table:0} erläuterten Dimensionen sind dabei nur eine der gängigen Modelle aus der Literatur. 
        \\
        Trotz des Bewusstseins für Datenqualität, besteht rund 80 Prozent der Arbeit eines Data Scientist daraus, Daten aufgrund von Qualitätsanforderungen vorzuverarbeiten.\cite{Horn2022} Angefangen beim einfachen Umwandeln bis hin zu komplexeren Bereinigungen und Decodierungen der Daten. Daten werden in den meisten Fällen nicht die gewünschten Qualitätsanforderungen erfüllen können. Datenqualität ist keine verallgemeinerbare Formel, sondern immer abhängig vom Anwendungsfall und dem Kontext in dem die Daten genutzt werden sollen.
        \\
        Dass Datenqualität eine wichtige Rolle spielt, unabhängig von dem Verwendungszweck, ist keine Frage. In der Wissenschaft sowie Wirtschaft wird sich immer intensiver mit der Thematik der Datenqualität auseinandergesetzte, da heutzutage die Quantität kam noch ein Problem darstellt, sondern viel mehr die Qualität der Daten.\cite{Lis2019} Denn Quantität ist nicht gleich Qualität!

%#########################################

    \newpage
    \section{\ac{KI}}
    \label{subsec:KIandML}
    \subsection{Künstliche Intelligenz allgemein}
    \label{subsubsec:KIAllgemein}
        \ac{KI} als Begriff wird vielseitig und unterschiedlich Verwendet. Die Vision von \ac{KI} ist es, die intellektuellen und menschlichen Fähigkeiten als ein KI-System nachzubilden.\cite{Lis2019} Letztendlich beschreibt \ac{KI} ein Forschungsbereich aus der Informatik, der aus einer Vielzahl aus Technologien besteht.\cite{HEGKI2019Definition} 
        \begin{figure}[h]
            \centering
            \includegraphics[width = \textwidth]{Bilder/ml_toolset.png}
            \caption{Exemplarisch dargestellte Disziplinen von \ac{KI}\cite{Horn2022}}
            \label{fig:ml_toolset}
        \end{figure}
        \\
        Die Abbildung \ref*{fig:ml_toolset} zeigt exemplarisch, welche Position \ac*{KI} in der Informatik einnimmt. Zur \ac{KI} gehören bekannte Teilgebiet wie das \ac*{ML}, Optimierungsalgorithmen, aber auch  in der Robotik kommt \ac{KI} zum Einsatz. Selbst wird die Disziplin aber auch von anderen übergeordneten Disziplinen genutzt.\cite{HEGKI2019Definition} Ein Beispiel hierfür ist Data Science. Die Abbildung \ref*{fig:ml_toolset} veranschaulicht, dass einige Aspekte aus der \ac{KI} im Bereich Data Science aufgegriffen werden, jedoch deutlich mehr als nur \ac{KI} hinter Data Science steckt.
        \\
        In der Literatur wurde in der Vergangenheit oft zwischen \glqq{}schwacher\grqq{} und \glqq{}starker\grqq{} \ac{KI} unterschieden. Eine schwache \ac{KI} ist dabei für ein spezifisches Anwendungsproblem konzipiert. Der \ac{KI} wird eine Aufgabe gegeben und diese versucht mittels Algorithmen und mathematischen Funktionen die Aufgabe zu lösen. Diese Art der \ac{KI} versucht Aufgaben zu lösen und Ergebnisse zu liefern, wie sie durch menschliche Intelligenz entstehen können.\cite{Lis2019} Ein Beispiel hierfür ist die Bilderkennung, bei der Entschieden wird, ob auf dem Bild eine Erdbeere zu sehen ist, oder nicht.
        \\
        Als starke \ac{KI} wird ein System verstanden, welches in der Lage ist ein breites Spektrum an Aufgaben zu erledigen.\cite{Datenkommission2019} Sie verfügt über weitere Methoden zur Datenverarbeitung. In dieser Form der \ac{KI} wird mehr versucht die menschlichen Fähigkeiten und Intelligenz abzubilden. Es wird nicht nach eine spezifischen Vorgehensweise vorgegangen, sondern Aufgaben werden mit individuellen Lösungswegen erledigt.\cite{Lis2019}
        \\
        Mit dem Begriff \ac{KI} wird demzufolge nur ein grundlegendes Prinzip von \ac*{KI}-Systemen definiert. Die Aufgabe dieser \ac*{KI}-Systeme ist es, Daten jeglicher Form zu interpretieren, Schlussfolgerungen daraus zu ziehen und eine Aussage über das zu erreichende Ziel zu liefern. Unabhängig davon, ob es sich um einen optimalen Lösungsweg, eine Vorhersage oder eine Schlussfolgerung handelt.\cite{HEGKI2019Definition}    
    
    \subsection{Teilgebiet maschinelles Lernen}
    \label{subsubsec:teilgebietML}
        Maschinelles Lernen ist das Teilgebiet der \ac{KI}, welches häufig als \ac*{KI} bezeichnet wird. Unter \ac{ML} wird verstanden, dass ein \ac*{ML} Modell von Lernalgorithmen entwickelt wird, welches in der Lage ist, das Wissen aus Erfahrungen anzuwenden. Dabei soll dem Modell ein unbekannter Input geliefert, dieses verarbeitet die Eingabewerte mit dem Wissen welches es besitzt und liefert ein Ergebnis als Output zurück. Der Output ist dabei abhängig von der zu lösenden Aufgabe. Klassische Anwendungsfälle von \ac*{ML} sind daher Vorhersagen, Schlussfolgerungen, Optimierungen, Entscheidungen, Sprach- und Bilderkennung \ac*{bzw} Verarbeitung und weitere ähnliche Aufgaben.\cite{HEGKI2019}
        \\
        Um das Teilgebiet \ac{ML} genauer zu betrachten, unterscheidet man in der Literatur zwischen \glqq{}superviced learning\grqq{}(\ac*{dt}.: überwachtes Lernen), \glqq{}unsuperviced learning\grqq{}(\ac*{dt}.: unüberwachtes Lernen) und dem \glqq{}reinforcement learning\grqq{}(\ac*{dt}.: bestärktes Lernen).\cite{Datenkommission2019} Diese Unterscheidung wird anhand der Art des Lernens getroffen.\cite{Horn2022} In Abbildung \ref*{fig:ml_algorithms}, werden diese unterschiedlichen Ansätze des Lernens vereinfacht dargestellt. 
        \begin{figure}[h]
            \centering
            \includegraphics[width = 14cm]{Bilder/ml_algorithms.png}
            \caption{Arten des maschinellen Lernen\cite{Horn2022}}
            \label{fig:ml_algorithms}
        \end{figure}
        \\
        Das \glqq{}unsuperviced learning\grqq{}(\ac*{dt}.: unüberwachtes Lernen), in der Abbildung \ref*{fig:ml_algorithms} links dargestellt, basiert auf dem Prinzip, dass dem Modell ein Input ohne zusätzliche Informationen gegeben wird und das Modell selbstständig Zusammenhänge erlernt. Lediglich die zu suchende Struktur ist dem Modell zu liefer, damit ist gemeint, ob nach bspw. Ausreißern gesucht werden soll. In der Abbildung  \ref*{fig:ml_algorithms}, wird dies an dem Beispiel eines Texts veranschaulicht. Dem Modell wird lediglich die Buchstabenkette gegeben und dieses erkennt darin die einzelnen Wörter und, dass es sich um eine Frage handelt. Ein häufiger Anwendungsfall für das unüberwachte Lernen ist das Clustering, da das Modell in den Daten Zusammenhänge erkennt, die ein Mensch nicht direkt sieht.\cite{Datenkommission2019}\cite{Döbel2018} \\
        Bei dem \glqq{}superviced learning\grqq{}(\ac*{dt}.: überwachtes Lernen), wird ein sehr Menschen ähnliches Lernverfahren genutzt. Vereinfacht gesagt wird bei dieser Art des Lernens etwas vorgemacht/erklärt und das Modell merkt sich diesen Zusammenhang. Auf diese Art und weiße erlernt das Modell Strukturen und kan dies auf ihm unbekannte Daten anwenden. Das Beispiel aus Abbildung \ref*{fig:ml_algorithms} veranschaulicht diesen Lernverfahren. Hier wird auf ein Flugzeug gezeigt und dem Kind gesagt, dass es sich um ein Flugzeug handelt. Das Kind merkt sich diesen Zusammenhang und ist beim nächsten mal selbst in der Lage das Flugzeug zu erkennen und es so zu benennen. Die gängigen Anwendungsfelder des überwachten Lernen sind Vorhersagen und Entscheidungen.\cite{Döbel2018} \\
        Die dritte und bisher am wenigsten verbreitete Art des Lernens ist das \glqq{}reinforcement learning\grqq{}(\ac*{dt}.: bestärktes Lernen). Es handelt sich dabei um Feedback basiertes Lernen. Es bedeutet, dass das Modell eine Auswahl an Handlungen hat, aus denen das Modell frei entscheiden darf, welche ausgeführt wird. Die Entscheidungen werden erst im Nachgang bewertet und in Form einer meist numerischen Bewertung an das Modell zurück gegeben. So lernt das Modell, wenn es etwas gut gemacht hat oder auch falsche Handlungen ausgeführt hat. In Abbildung \ref*{fig:ml_algorithms}, wird dies anhand einer musizierenden Person dargestellt. Die Person spielt etwas auf ihrem Instrument und bekommt nach dem Spielen eines Tons das Feedback der Zuhörenden ob es sich gut anhört oder nicht. So lernt die Person wann sich etwas gut anhört und verbessert durch das Feedback ihr können. Häufig findet das bestärkende Lernen deshalb Einsatz in der Robotik, in der ein Roboter anhand von Feedback Bewegungsabläufe erlernt. Bekannt ist hier eine Roboter, der eine menschliche Hand nachbildet und erlernt einhändig einen Rubik’s Cube zu lösen.\cite{Horn2022}\cite{Döbel2018}\cite{Rubik2019}
        \\
        Im Rahmen dieser Arbeit wird der Fokus speziell auf das superviced Learning gelegt. Denn überwachtes Lernen komm meist dann zum Einsatz, wenn Vorhersagen oder Entscheidungen getroffen werden. Dieser Aspekt wird im Verlauf der Arbeit immer wieder eine Rolle spielen. Einzigartig am überwachten Lernen ist, wie bereits erwähnt, dass zum Lernen von Zusammenhängen immer ein \glqq{}Lehrer\grqq{} benötigt wird. Im Falle eines \ac{ML} Modells handelt es sich dabei nicht um eine Lehrer, sondern um Daten, genauer gesagt Trainingsdaten.
        Das Prinzip nach dem das überwachte Lernen funktioniert, ist in Abbildung \ref*{fig:learningProcess} exemplarisch anhand eines Beispiels dargestellt.
        \begin{figure}[h]
            \centering
            \includegraphics[width = 14cm]{Bilder/superviced_learning.png}
            \caption{Der Prozess von überwachtem Lernen \cite{Kharwal2020}}
            \label{fig:learningProcess}
        \end{figure}
 %       ZU ÜBERARBEITEN
        \\
        Zu Beginn wird das zu lösende Problem definiert, dabei geht es darum die Lernaufgabe des Modells festzulegen. Beim überwachten Lernen wird zwischen Regressionen und Klassifikation unterschieden. Regressionen kommen meist bei Schlussfolgerungen oder Vorhersagen zum Einsatz und Klassifikationen bei Entscheidungen. 
        Anhand des Problems wird dann ein Algorithmus als Grundlage für das Modell ausgewählt. Um den Modell nun Wissen beizubringen werden sogenannte Trainingsdaten verwendet. Dabei handelt es sich um Daten in dem Format, wie sie auch später im Betrieb als Eingaben X geliefert werden. Diese werden jedoch davor mit Labels versehen. Diese Labels werden durch Experten für die Trainingsdaten erstellt. Im nächsten Schritt wird dann das Modell auf Basis der Trainingsdaten trainiert. Diese Phase ist die entscheidende Lernphase des Modells. Im Prinzip ist das Modell nach dem training bereit für den Einsatz. Meist wird jedoch noch mit einem, dem Modell bisher unbekannten und nicht gelabelten, Datensatz überprüft ob das Modell richtig funktioniert und die Qualität anhand unterschiedlicher Metriken, wie der Genauigkeit bewertet. Danach ist das Modell Einsatzbereit und kann mit Eingaben X gespeist werden und schließt dann auf Basis des erlernten Wissens auf eine Ausgabe Y.\cite{Horn2022}\cite{Döbel2018}
        Auf diese Art und weiße werden beim überwachten Lernen die input Output Probleme gelöst.\cite{Ng2018}
        \\
        Es gibt aber auch einige negative Eigenschaften die bei \ac{ML} Modelle zu berücksichtigen sind und zu möglichen Problemen führen können. Die meisten dieser Risiken lassen sich jedoch inzwischen durch Gegenmaßnahmen verhinder. Zu den Häufigsten Problemen gehören zu geringe Datenmengen, die Passgenauigkeit, Scheinkorrelationen und Verzerrungen. 
        \begin{enumerate}
            \item Die Datenmenge ist ein entscheidender Faktor im überwachten Lernen. Viele Modelle scheitern daran, dass sie auf Basis von zu wenigen Trainingsdaten trainiert wurden und so nicht in der Lage sind treffende Schlüsse zu ziehen.\cite{Datenkommission2019}\cite{Ng2018}
            \begin{figure}[h]
                \centering
                \includegraphics[width = 8cm]{Bilder/Datenmenge.png}
                \caption{Veränderung der Performance durch die Datenmengen \cite{Ng2018}}
                \label{fig:Datavolume}
            \end{figure} 
            In Abbildung \ref*{fig:Datavolume} ist der Zusammenhang zwischen der Datenmenge und der Performance des Modells dargestellt. Hier wird zwar des spezielle Fall von unterschiedlich großen Neuronalen Netzen betrachtet, jedoch lässt sich dies auch auf allgemein \ac*{ML} übertragen. Der Verlauf ähnelt dem eines beschränkten Wachstums. Das bedeutet für das Modell, dass es zu Beginn mit einer ausreichenden Datenmenge möglich ist eine gute Performance zu erzielen, jedoch um später wenige Prozente an Verbesserung zu erzielen werden sehr große Datenmengen benötigt.\cite{Ng2018} In einigen Szenarien existieren aber von Beginn an zu wenig Daten für eine gute Performance. Als Gegenmaßnahme können lediglich mehr Daten generiert und gesammelt werden.
            \item Bei der Passgenauigkeit wird ein Problem bei dem Modell selbst adressiert, nämlich das overfitting (\ac*{dt}.: \glqq{}überangepasst\grqq{}) und underfitting (\ac*{dt}.: \glqq{}unterangepasst\grqq{}). Vereinfacht gesagt, wird beim overfitting der Trainingsdatensatz durch das Modell zu genau repräsentiert. Erkennbar wird overfitting, wenn man die Performance von dem Modell bei einem Trainings- und eine Testdatensatz betrachtet. Ist die Performance bei den Trainingsdaten akzeptabel und bei den Testdaten deutlich schlechter, ist es nahezu sicher, dass das Modell zu sehr auf die Trainingsdaten angepasst ist.
            Beim underfitting hingegen, ist die Performance bei beiden Datensätzen nicht akzeptabel. Das liegt daran, dass beim underfitting zu wenig Zusammenhänge erlernt wurden und so keine realistischen Ergebnisse reproduziert werden können.
            \begin{figure}[h]
                \centering
                \includegraphics[width = 12cm]{Bilder/overunderfitting.png}
                \caption{Overfitting und Underfitting \cite{Horn2022}}
                \label{fig:Fitting}
            \end{figure}
            Diese Problematik wird in Abbildung \ref*{fig:Fitting} veranschaulicht. Auf der linken Seite ist ein über angepasstes Modell, welches den Trainingsdatensatz \glqq{}auswendig gelernt\grqq{} hat und nicht auf neue Datenpunkte verallgemeinern kann. Rechts ist das unter angepasste Modell zu sehen, welches den Zusammenhang von Input und Output nicht abbilden kann und zu ungenau ist, da es \glqq{}zu wenig gelernt\grqq{} hat.\cite{Horn2022}
            \item Nicht nur overfitting und underfitting sorgen in Modellen für Ungenauigkeiten auch Scheinkorrelationen sind häufig eine Fehlerursache. Bei Scheinkorrelationen, werden von dem Modell Zusammenhänge erlernt, die keinerlei Zusammenhang mit der tatsächlichen Entscheidung haben.
            \begin{figure}[h]
                \centering
                \includegraphics[width = 12cm]{Bilder/Scheinkorrelation.png}
                \caption{Scheinkorrelationen in der Bilderkennung \cite{Lapuschkin2016}}
                \label{fig:Scheinkorrelation}
            \end{figure} \\
            Ein Beispiel dafür ist in Abbildung \ref*{fig:Scheinkorrelation} zu sehen. In der ein Pferdebild nicht anhand der Kontur des Pferdes, sondern anhand des Wasserzeichens in dem Bild erkannt wurde. Grund dafür war, dass einige der Trainingsdaten dieses Wasserzeichen beinhaltet haben und das Modell daraus eine Scheinkorrelation erlernt hat.\cite{Horn2022}\cite{Lapuschkin2016}\cite{Cremers2019}
            \item Ein letztes und schwerwiegendes Problem ist die Verzerrung von Trainingsdaten, auch Bias genannt. Dieser Aspekt wird später in Kapitel \ref*{subsubsec:Bias} genauer betrachtet. Allgemein kann man sagen, existieren Verzerrungen in Trainingsdaten, so wird das Modell diese Verzerrungen reproduzieren.\cite{Cremers2019}
        \end{enumerate}
        Zusammenfassend wird bei \ac*{ML}, speziell beim überwachten Lernen auf Basis von Daten, Trainingsdaten genannt, Wissen aus der Vergangenheit erlernt. Dazu werden Zusammenhänge in den Daten genutzt und die dazugehörigen Bewertungen in Form von Labels. So entsteht eine Abhängigkeit des Modells von den Trainingsdaten. Im Falle von zu wenig Trainingsdaten, wird das Modell die Realität unterrepräsentieren und daher nicht einsetzbar. Die Daten selbst können aber ebenfalls zu Scheinkorrelationen und Verzerrungen führen die das Ergebnis des Modells mitunter stark beeinflussen.\cite{Cremers2019}
 
    \newpage
    \subsection{Ethik in der künstlichen Intelligenz}
    \label{subsubsec:ethikinderKI}
        Ein Aspekt der bisher nicht beleuchtet wurde ist die Ethik. Die Ethik spielt in der \ac{KI} eine immer größer werdende Rolle. In einer Gesellschaft die für Inklusion steht, werden häufig Entscheidungen hinterfragt und kritisiert. Dies gilt insbesondere für Entscheidungen und Vorhersagen, die durch \ac{KI} getroffen werden und so ohne direkten menschlichen Einfluss getroffen werden.
        \begin{figure}[h]
            \centering
            \includegraphics[width = 14cm]{Bilder/Gartner_hypeCycle.png}
            \caption{Gartner Hype Cycle für \ac*{KI} \cite{Goasduff2020}}
            \label{fig:HypeCycle}
        \end{figure}
        \\
        Im Gartner Hype Cycle für \ac*{KI} aus dem Jahr 2020, dargestellt in Abbildung \ref*{fig:HypeCycle}, wurde das Thema Digitale Ethik auf dem Höhepunkt der Kurve platziert. Der Forschungsbereich rund um die digitale Ethik befindet sich jedoch erst im Anfangsstadium. Trotzdem sind die Erwartungen bereits sehr hoch, denn auch der gesellschaftliche Druck steigt an. Ursache dafür ist unter anderem der Skandal um den Chatbot Tay von Microsoft der innerhalb von nur einem einzigen Tag rassistische \"Außerungen von den Nutzern lernte und daraufhin offline genommen werden musste.\cite{Cremers2019}
        Um einen einheitlichen Rahmen für die Ethik in der \ac*{KI} zu schaffen wurde 2018 von der EU eine Kommission von hochrangigen \ac*{KI} Experten (kurz. HEG KI) gegründet, aber auch Forschungsinstitute setzten sich vermehrt mit der Thematik auseinander. 2021 waren es allein in Deutschland über 40 Projekte.\cite{Beckert2021}
        \\
        Grundlegend ist die Aufgabe der Ethik in der \ac*{KI}, dass ein Rahmen geschaffen wird, in dem eine \ac*{KI} agiert und dabei keine ethischen Grundprinzipien verletzt.\cite{Cremers2019} Ethik ist jedoch nicht einfach in Form von Code programmierbar und kann daher auch nicht von Grund auf in eine \ac*{KI} integriert werden. In den meisten Fällen ist Ethik und ethische Entscheidungen stark von dem Rahmen und Kontext abhängig. Ziel der Ethik ist es sich so zu verhalten und zu handeln, dass  keine beteiligte Person in ihren Rechten beschnitten wird oder einen Schaden erleidet.\cite{Heesen2020}
        Dabei bezieht sich die Ethik für die Vorgabe von Leitlinien meist auf die Grundrechte die in Deutschland durch das Grundgesetz definiert werden. In Zeiten der Digitalisierung reichen diese ethischen Grundsätze jedoch oft nicht aus. Mit immer schneller aufkommenden neun Technologien kommen auch neue Probleme und nicht selten auch ethische Probleme auf. Da die Ethik aber auf historischen Erfahrungen basiert, fehlen diese, um einen klaren Weg vorzugeben. So kommt es, dass die Ethik mehr und mehr eine zentrale Rolle in der Gesellschaft gewinnt, wenn es um den Umgang mit neuen digitalen Technologien geht.\cite{Cremers2019} \\
        Die \ac*{KI} hat die Frage nach Ethik dabei insbesondere damit geprägt, dass Entscheidungen von einem maschinellen Modell getroffen werden, welche in der Vergangenheit oft nicht transparent und nachvollziehbar waren. Für Viele fehlt daher das Vertrauen in eine Entscheidung, die durch einen Computer getroffen wurde. Um diese Bedenken zu minimieren und ein Vertrauen zu schaffen benötigt man Leitlinien für Ethik in der \ac*{KI}.\cite{Heesen2020}
        \\
        Die Ethik wird dabei meist mit dem Konzept von vertrauenswürdiger \ac*{KI} in Verbindung gebracht. In diesem Konzept werden die folgenden drei Anforderungen an eine \ac*{KI} gestellt:
        \begin{enumerate}
            \item \textbf{Rechtmäßigkeit:} \\ Eine \ac*{KI} soll das geltenden Recht und die gesetzlichen Bestimmungen einhalten.
            \item \textbf{Ethik:} \\ Eine \ac*{KI} soll ethische Grundsätze und Werte einhalten.
            \item \textbf{Robustheit:} \\ Eine \ac*{KI} soll technisch Robust gegen Manipulation gestaltet sein.
        \end{enumerate}
        Erfüllt eine \ac*{KI} alle drei Anforderungen gilt sie als eine vertrauenswürdige \ac*{KI}. Ziel des Konzepts ist es, die Vorteile von \ac*{KI} zu maximieren aber gleichzeitig die Risiken, egal ob rechtliche oder ethische, möglichst gering zu halten.\cite{HEGKI2019} Dafür muss eine Grundlage auf Basis der drei Kategorien Recht, Ethik und Technologie geschaffen werden, die nur im Zusammenspiel zu einer umsetzbaren vertrauenswürdigen \ac*{KI} führen.
        \\
        Wenn man sich nun mit dem Konzept einer vertrauenswürdigen \ac*{KI} genauer beschäftigt, müssen Fragestellungen, wie sie in Abbildung \ref*{fig:EthikAnforderungen} zu sehen sind, gestellt werden. 
        \\
        \begin{figure}[h]
            \centering
            \includegraphics[width = 14cm]{Bilder/Grafik-KI_Whitepaper_final_WEB_2.png}
            \caption{Ethische und rechtliche Grundlagen für eine \ac*{KI} \cite{Cremers2019}}
            \label{fig:EthikAnforderungen}
        \end{figure}
        \\
        Die Abbildung \ref*{fig:EthikAnforderungen} stellt eine Auswahl konkreter Aspekte, der oben beschriebenen Grundsätze, dar. Es sind die einzelnen Aspekte die für eine vertrauenswürdige \ac*{KI} notwendig sind. Begonnen bei ethischen Grundsätzen wie der Autonomie und Fairness bis hin zu klar rechtlich zuzuordnenden Grundsätzen wie der Sicherheit und dem Datenschutz.\cite{Cremers2019}
        \begin{enumerate}
            \item \textbf{Autonomie und Kontrolle} \\
            Unter Autonomie und Kontrolle versteht man im Bezug auf die \ac*{KI} die Selbstbestimmtheit. Grundlage der Selbstbestimmtheit ist, dass jedes Individuum Entscheidungen frei und selbstbestimmt treffen kann. Der Konflikt mit der \ac*{KI} entsteht durch die Beeinflussung von Entscheidungen durch Entscheidungsvorschläge oder Vorhersagen. Durch das Vertrauen auf die \ac*{KI} kann es zu einer Beeinträchtigung im Entscheidungsprozess kommen. Die Autonomie fordert deshalb die Selbstbestimmtheit der Nutzenden sowie die Transparenz über Risiken und mögliche Beeinträchtigungen in der Autonomie.\cite{Cremers2019}\cite{Heesen2020}
            \item \textbf{Fairness} \\
            Fairness ist ein grundlegendes Prinzip in der Gesellschaft. Dabei geht es um den Gleichbehandlungsgrundsatz, der besagt, dass es keine ungerechtfertigte Ungleichbehandlung geben darf. Dieses Prinzip muss auch von einer \ac*{KI} erfüllt werden, damit eine vertrauenswürdige Basis einer \ac*{KI} geschaffen werden kann. Es ist darauf zu achten, dass es keine zu unrecht existierenden Vorurteile oder Diskriminierungen gibt. In diesem Kontext spricht man bei \ac*{KI} von Bias \glqq{}\ac*{dt}.: Vorurteil\grqq{}. Ein Bias entsteht durch die Trainingsdaten im \ac*{ML} und sorgt für eine unzulässige Ungleichbehandlung durch die \ac*{KI}. Das Thema Bias und Diskriminierung durch Bias wird in den Kapiteln \ref*{subsubsec:Bias} und \ref*{subsubsec:diskriminierungdurchverzerrung} nochmals genauer betrachtet.\cite{Cremers2019}\cite{hagendorff2021blind}
            \item \textbf{Transparenz} \\
            Mit der Transparenz ist in erster Linie die Transparenz der \ac*{KI} gemeint. Der Fokus liegt darauf, dass es für ein vertrauenswürdigen Umgang mit \ac*{KI} notwendig ist, nachzuvollziehen wie Entscheidungen getroffen werden. Der Bedarf von Transparenz ist besonders hoch, da Entscheidungen Unfair erscheinen können und eine Entscheidungsgrundlage Klarheit liefern kann. Technologisch handelt es sich jedoch bei den meisten Modellen um sogenannte Black-Box Modelle welche in ihrer Entscheidung nicht nachvollziehbar sind. Trotzdem wird von einer transparenten \ac*{KI} die Interpretierbarkeit, die Nachverfolgbarkeit sowie die Reproduzierbarkeit des Ergebnisses erwartet.\cite{Cremers2019}\cite{Hallensleben2020}\cite{Heesen2020}
            \item \textbf{Verlässlichkeit} \\
            Die Verlässlichkeit ist die ein Aspekt der sich mit der tatsächlichen Funktion der \ac*{KI} befasst. Sie umfasst die Korrektheit der Ausgabe aber auch die technologische Robustheit. Für die Korrektheit ist die Implementierung des Modells sowie die verwendeten Trainingsdaten ausschlaggebend. Bei der technologischen Robustheit wird die Anfälligkeit für Fehler und Manipulationen adressiert. Insbesondere Adversial oder Poison attacks spielen bei der Gewährleistung von einer verlässlichen Funktion ein Risiko dar, da sie das Verhalten einer \ac*{KI} manipulieren können. Die Korrektheit durch Fehler oder Manipulationen gefährdet werden und so auch die Verlässlichkeit der \ac{KI}. Dabei ist die Verlässlichkeit die Grundlage für den Einsatz von \ac*{KI} und daher sowohl während der Entwicklung aber auch dem produktiven Einsatz zu jedem Zeitpunkt zu gewährleisten.\cite{Cremers2019}\cite{Hallensleben2020}
            \item \textbf{Sicherheit} \\
            Bei der Sicherheit ist nicht die der \ac*{KI} selbst gemeint, denn diese ist Bestandteil der Verlässlichkeit. Die Sicherheit bezieht sich auf das gesamte System in dem sich die \ac*{KI} und deren Komponenten befinden. Dabei sollen mögliche Schwachstellen für den Missbrauch der \ac*{KI} geschützt werden aber auch allgemein die Sicherheitsanforderungen der gängigen Zertifizierungen wie der ISO Norm 27001 für die allgemeine IT-Sicherheit.\cite{Cremers2019}\cite{Hagendorff2020}
            \item \textbf{Datenschutz} \\
            Um die Privatsphäre und so das Recht auf informationelle Selbstbestimmung gewährleisten zu können wird Datenschutz benötigt. Auch eine \ac*{KI} muss mit Datenschutzrichtlinien konfrontiert werden. Häufig wird mit sensiblen Daten, egal ob personenbezogene Daten oder Geschäftsgeheimnisse, gearbeitet die es sowohl rechtlich als auch ethisch betrachtet zu schützen gilt. Die Daten ermöglichen es nämlich selbst in teils anonymisierter From Rückschlüsse zu ziehen. Eine \ac*{KI} is daher verpflichtet die rechtlichen Bestimmungen der Datenschutz-Grundverordnung (DSGVO) und des Bundesdatenschutzgesetz (BDSG) einzuhalten.\cite{Cremers2019}
        \end{enumerate}
        Mit den sechs Aspekten die in Abbildung \ref*{fig:EthikAnforderungen} aufgeführt sind, werden jedoch nur die grundlegendsten Anforderungen an \ac*{KI} Systeme näher betrachtet. Neben diesen Anforderungen gibt es noch eine Vielzahl weiterer Aspekte die in einem bestimmten Kontext zwingend erforderlich sind. 
        \begin{figure}[h]
            \centering
            \includegraphics[width = \textwidth]{Bilder/Ethical huidelines hagendorf 2020.png}
            \caption{Übersicht ethischer Leitlinien und die Abgedeckten Aspekte \cite{Hagendorff2020}}
            \label{fig:EthikGuidelines}
        \end{figure} \\
        In der Abbildung \ref*{fig:EthikGuidelines} werden unterschiedliche Leitlinien für vertrauenswürdige \ac*{KI} verglichen. Für den Vergleich werden die unterschiedlichen betrachteten Aspekte der einzelnen Leitlinien aufgeführt. Die daraus entstehende Tabelle veranschaulicht dann, welche Leitlinie welche Aspekte beinhaltet. Zudem wurden die Aspekte der Häufigkeit nach absteigend sortiert. Das bedeutet, die oben stehenden Aspekte werden am häufigsten in Leitlinien betrachtet. Dabei stellt sich heraus, dass einige bereits erwähnte Aspekte wie die Fairness, Sicherheit und Verlässlichkeit in nahezu allen Leitlinien ein Bestandteil sind. Sie sind daher auch in \ref*{fig:EthikGuidelines} bei den obersten Punkten dabei. Es wird aber ebenso der Konflikt mit der Erklärbarkeit und der Selbstbestimmtheit bzw. Autonomie veranschaulicht. Denn die Erklärbarkeit wird lediglich in 10 Leitlinien berücksichtigt und die Autonomie in nur 7 von gesamt 22 Leitlinien. Des Weiteren werden neu aufkommende Problematiken wie die Diversität im Umfeld von \ac*{KI} bisher nahezu gar nicht betrachtet.\cite{Hagendorff2020}\cite{jobin2019global} 
        \\
        Die Ethik spielt bei dem Einsatz von \ac*{KI} eine immer größer werdende Rolle. Ethik allein reicht jedoch nicht aus um eine vertrauenswürdige \ac*{KI} zu schaffen. Es ist ein zusammenkomme von ethischen, rechtlichen und technologischen Herausforderungen, die es zu lösen gilt. Um ein einheitliches Verständnis von vertrauenswürdiger \ac*{KI} zu schaffen, werden Zertifizierungen benötigt. Nur so ist es möglich den Einsatz von \ac*{KI} auf einer vertrauenswürdigen Basis zu schaffen. Dabei müssen die grundlegenden Anforderungen wie Fairness, Autonomie, Sicherheit, Verlässlichkeit, Transparenz und Datenschutz immer erfüllt sein. Es kann zusätzlich noch weitere Anforderungen geben, die sich aufgrund der Rahmenbedingungen, dem Kontext oder den verarbeiteten Daten ergeben können.\cite{Cremers2019}\cite{Hagendorff2020}

%#########################################
    \newpage
    \section{Vorurteile im Zusammenhang mit \ac{KI}}
    \label{subsec:KIundbias}
    \subsection{Bias}
    \label{subsubsec:Bias}
        Als Bias versteht man in der Literatur allgemein eine Verzerrung eines Wertes oder die Abweichung von einem Standard.\cite{Fabi2022} Der Begriff stammt ursprünglich aus dem Englischen und ist auf Deutsch übersetzt gleichbedeutend mit den Worten: Vorurteil, Voreingenommenheit, Neigung und Tendenz. In der \ac*{KI} versteht man unter Bias ebenfalls eine Verzerrung. Dabei bezieht sich die Verzerrung jedoch meist auf die Ausgabe einer \ac*{KI}, also der Ausgabe von \ac*{ML} Algorithmen.\cite{Dilmegani2020} Der Bias selbst ist dabei unabhängig von einer positiven oder negativen Verzerrung.\cite{silberg2019notes}\cite{ntoutsi2020bias}
        Diese Verzerrungen durch einen Bias werden in das Konzept der vertrauenswürdigen \ac*{KI}, aus Kapitel \ref{subsubsec:ethikinderKI}, in den Aspekt der Fairness eingeordnet. Fairness selbst, ist ein elementarer Bestandteil der Ethik in der Gesellschaft. Etwas gilt als \glqq{}fair\grqq{}, wenn keine unzulässige Ungleichbehandlung stattfindet. Im Umkehrschluss gilt etwas als \glqq{}unfair\grqq{}, wenn es eine Gruppe oder Individuen aufgrund von Eigenschaften ungerechtfertigt systematisch Diskriminiert.\cite{silberg2019notes}
        \\
        Für die Arbeit wird speziell der Bias im \ac*{ML} betrachtet. Schwerpunkt wird dabei auf den negativen Einfluss von Verzerrungen im \ac*{ML} und die Auswirkungen eines Bias auf die Ausgabe gelegt. Ein Bias im \ac*{ML} kann aus unterschiedlichen Gründen zustande kommen. Dabei wird in der Literatur oft eine Unterscheidung in die drei Bereiche Bias durch Daten, Bias durch Algorithmen und Bias durch Menschen gemacht.\cite{srinivasan2021biases}\cite{Mehrabi2021}
        \begin{figure}[h]
            \centering
            \includegraphics[width =15cm]{Bilder/Bias_cyrcle.png}
            \caption{Exemplarische Arten von Bias in den drei Teilbereichen Daten, Algorithmen und Mensch \cite{Mehrabi2021}}
            \label{fig:BiasCycle}
        \end{figure}\\
        Eine eindeutige Unterteilung fin diese drei Bereiche ist jedoch schwer möglich. Meist beeinflussen sich die Bereiche gegenseitig, wie es in der Abbildung \ref*{fig:BiasCycle} dargestellt ist. Zum Beispiel eine Verzerrung die durch einen Menschen zustande kommt, wird Einfluss auf die Daten haben, die bei der Entwicklung der \ac{KI} eine elementare Rolle spielen. Aber Ebenso ist es unter Umständen möglich, dass die \ac{KI} zuvor, durch seine Ausgabe, einen Einfluss auf das Handeln eines Menschen gehabt hat.\cite{Mehrabi2021} Ein Bias ist demnach also entweder in den Daten, einem Algorithmus oder einem Menschen zu verorten. Seine Auswirkungen bzw. seine Verzerrung wird aber erst in dem danach folgenden Bereich sichtbar, wie es in Abbildung \ref*{fig:BiasCycle} zu sehen ist. Der Bias uns seine Verzerrung in der \ac*{KI} bilden so eine Kreislauf der in allen Bereichen seinen Einfluss hat.\cite*[]{Mehrabi2021}
        Bias ist aber nur der Überbegriff. In den drei benannten Kategorien gibt es eine Vielzahl an unterschiedlichen Ausprägungen von Bias. Deshalb werden im folgenden die Kategorien in denen Bias vorkommen und die dazugehörigen spezifische Ausprägungen von Bias genauer untersucht.
        \begin{description}
            \item [Bias durch Daten:]\hfill \\
            Der Bias durch Daten hat seinen Ursprung in der Funktionsweise von \ac{ML}. Wie bereits in Kapitel \ref{subsubsec:teilgebietML} genauer beschrieben, wird beim \ac{ML} mit Trainingsdaten gearbeitet. Speziell beim überwachten \ac*{ML} werden historische Daten genutzt um Zusammenhänge und Strukturen aus Daten zu erlernen. So wird eine Verhaltensweise, meist die eines Menschen, durch ein Modell nachgebildet. Entscheidend für die Funktionalität sind daher die Trainingsdaten. Sie sind auch der Ort an dem sehr häufig ein Bias auftritt.\cite{silberg2019notes}\cite{Drew2019}
            \\
            Die wichtigsten Ausprägungen von Bias in Daten sind: Measurement Bias, Sampling Bias und Label Bias\cite{srinivasan2021biases}\cite{Mehrabi2021}
            \begin{description}
                \item [Measurement Bias:] \hfill \\
                Der \glqq{}measurement bias\grqq{} (\ac*{dt}.:Verzerrung der Messung) entsteht durch die Art in der Merkmale ausgewählt, verwendet und ermittelt werden. Ein Beispiel für diese Art der Verzerrung ist, dass für das Kriminalitätsrisiko Bewertung die Verhaftungsrate im Freundes-/Familienkreis herangezogen wurden. So entstand der Trugschluss, dass Minderheitengruppen aufgrund einer höheren Verhaftungsrate gefährlicher seine. Zurückzuführen ist der Bias auf die häufigere Kontrolle von Minderheitengruppen und eine dadurch verursachte höhere Verhaftungsrate.\cite{silberg2019notes}\cite{srinivasan2021biases}\cite{Mehrabi2021}
                \item [Sampling Bias:] \hfill \\
                Ein \glqq{}sampling bias\grqq{} (\ac*{dt}.: Stichprobenverzerrung) ist eine der häufigsten Arten von Verzerrungen in Datensätzen. Hier entsteht die Verzerrung dadurch, dass eine bestimmte Gruppe wie beispielsweise dunkelhäutigen Personen im Vergleich zu hellhäutige Personen unter repräsentiert wird oder vollständig fehlt. Wenn Gruppen im unterrepräsentiert sind oder gar ganz in den Trainingsdaten fehlen, kann dies dazu führen, dass der Algorithmus schlechter generalisiert und so schlechter gegenüber dunkelhäutigen Personen funktioniert. Die Stichprobe in Form der Trainingsdaten ist, wenn ein Sampling Bias enthalten ist aus diesem Grund nicht repräsentativ für die reale Welt und somit auch nicht als Trainingsdatensatz für \ac*{ML} geeignet.\cite{srinivasan2021biases}\cite{Mehrabi2021}
                \item [Lable Bias:] \hfill \\
                \glqq{}Lable bias\grqq{} (\ac*{dt}.: Verzerrung der Beschriftung) bedeutet, dass es eine Verzerrung in der Bewertung der Trainingsdaten gibt. Zurückzuführen ist dieser Bias auf seine Entstehung, genauer auf die bewertenden Personen. Die Verzerrung wird durch die individuelle Begebenheiten verursacht. Es kann z.B. aufgrund unterschiedlicher Herkunft passieren, dass der selbe Objekttyp mit unterschiedlichen Bezeichnungen verstehen wird. Als Beispiel, das Bild einer Frikadelle wird je nach Herkunft des Bewertenden möglicherweise auch mit dem Namen: Fleischküchle, Fleischpflanzerl oder Bulette benannt.
                Durch die Verzerrung der Bewertung können aber ebenso die Vorurteile von Menschen in den Daten repräsentiert werden. So kann beispielsweise das diskriminierende Handeln in Form der Bewertung von Entscheidungen oder Bewertungen in die Trainingsdaten integriert werden. Auf diese Weise können unzulässige Muster in eine \ac*{KI} übertragen werden.\cite{srinivasan2021biases}
            \end{description}
            
            \item [Bias durch Algorithmen:] \hfill \\
            Nicht nur die Daten die zum Trainieren und Lernen von \ac*{ML} Modellen genutzt werden können die Ursache einer Verzerrung sein. Auch das Modell selbst kann für eine Verzerrung sorgen. Beispiele für einen Bias durch Algorithmen sind: Algorithmic Bias und Confounding Bias.
            \begin{description}
                \item [Algorithmic Bias:] \hfill \\
                Von einem \glqq{}algorithmic bias\grqq{}(\ac*{dt}.: Algorithmische Verzerrung) spricht man, wenn die Eingabedaten keinen Bias enthalten, sondern der Algorithmus eine Verzerrung hinzufügt. Dies kann durch verschiedene Entscheidungen wie die Regularisierung, Optimierungsfunktionen oder die Anwendung von Regressionsmodellen zustande kommen und die Entscheidungen des Algorithmus verzerren.\cite{srinivasan2021biases}\cite{Mehrabi2021}
                \item [Confounding Bias:] \hfill \\
                Der \glqq{}confounding bias\grqq{} (\ac*{dt}.: Verdeckende Verzerrung) ist die Folge aus falsch erlernten Zusammenhängen. Zum einen können Beziehungen in Daten erlernt werden, die nicht korrekt sind. Zum anderen kann es auch sein, dass bestehende Beziehungen in den Daten von dem Algorithmus nicht erlernt werden und so bei dem Einsatz des Modells nicht zu erwartende Verzerrungen auftreten. Diese Art des Bias hängt mit dem Problem des underfitting im \ac*{ML} zusammen, denn es werden unzureichende Modellanpassungen. Der confounding bias kann daher eine Ursache für underfitting von \ac*{ML} Modellen sein.\cite{srinivasan2021biases}
            \end{description}
                
            \item [Bias durch Menschen:] \hfill \\
            Dieser Aspekt der Verzerrung von Ergebnissen wird in der Arbeit jedoch nicht tiefer gehend betrachtet, da es sich mehr um ein ethisches Problem der Gesellschaft handelt, als speziell um ein Konflikt in der Nutzung von \ac{KI}. Exemplarisch werden trotzdem der Historical Bias, Social Bias und der Behavior Bias erläutert. Diese haben zwar keinen direkten Einfluss auf \ac*{KI} und \ac*{ML}, indirekt beeinflussen sie jedoch die Daten die letzten Endes für das \ac*{ML} genutzt werden.
            \begin{description}
                \item [Historical Bias:] \hfill \\
                Als \glqq{}historical bias\grqq{} (\ac*{dt}.: Historische Voreingenommenheit) versteht man grundlegend Vorurteile. Speziell die Voreingenommenheit von Personen unabhängig von Daten. Diese Vorurteile können zu Diskriminierung führen und auch im Bereich von \ac*{ML} eine Rolle bei der Erstellung von Test bzw. Trainingsdaten spielen. Historische Voreingenommenheit ist somit der Grund aus dem bspw. ein Lable Bias entsteht.\cite{Mehrabi2021}\cite{Parkavi2018}
                \item [Social Bias:] \hfill \\
                \glqq{}Social bias\grqq{} (\ac*{dt}.: Soziale Vorurteile) beschreibt die Manipulation in der menschlichen Entscheidungsfindung. Egal ob durch Rezensionen, Reviews oder Bewertungen, Menschen lassen sich in ihrem Handeln durch die Gesellschaft beeinflussen. Eine schlechte Bewertung eines Artikels führt in der Regel zu dem Vorurteil, dass der Artikel nicht gut ist und führt dazu sich gegen den Kauf zu entscheiden.\cite{Mehrabi2021}\cite{Parkavi2018}
                \item [Behavior Bias:] \hfill \\
                Der \glqq{}behavior bias\grqq{} (\ac*{dt}.: Verhaltensbedingte Vorurteile) ergibt sich aus einem unterschiedlichen Nutzungsverhalten. Abhängig von dem Kontext und den Rahmenbedingungen, ändert sich auch das Nutzungsverhalten der Nutzenden. Es kann beispielsweise gegen die Erwartung passieren, dass die gleiche Handlung auf unterschiedlichen Plattformen zu unterschiedliche Reaktionen führt.\cite{Mehrabi2021}
            \end{description}
        \end{description}
        Verzerrungen können in jedem Prozessschritt beim \ac*{ML} entstehen. Egal ob bei der Erstellung, Auswahl oder Bewertung von Daten oder bei der Verwendung von Algorithmen. Entscheidend ist, wie man mit dieser Verzerrung umgeht und ob man sich dieser Bewusst ist. Ziel muss es sein, einen Rahmen für den vertrauenswürdigen Einsatz von \ac*{KI} zu gewährleisten. Zu diesem gehört die Gerechtigkeit und Fairness so wie die Autonomie des Menschen. 
        Letztendlich wird unabhängig von der Ursache des Bias eine Verzerrung des Ergebnisses verursacht. Dabei werden Freiheiten von Personen eingeschränkt und gegen die Anforderungen der Fairness und Autonomie, wie es durch die vertrauenswürdige \ac*{KI} gefordert wird, verstoßen. Der Aspekt der Diskriminierung wird im folgenden Kapitel \ref*{subsubsec:diskriminierungdurchverzerrung} nochmals anhand von Beispielen betrachtet.\cite{HEGKI2019Definition}\cite{Fabi2022}

    \subsection{Diskriminierung durch Vorurteile in Daten}
    \label{subsubsec:diskriminierungdurchverzerrung}
        Diskriminierung durch \ac*{KI} sorgt in der Öffentlichkeit immer wieder für Schlagzeilen.\cite{Cremers2019} Und das obwohl Experten in der \ac*{KI} eine Möglichkeit sehen, die Vorurteile des Menschen zu reduzieren indem die Entscheidungsfindung durch einen Algorithmus übernommen wird. Das Risiko, dass eine \ac*{KI} jedoch die menschlichen bzw. gesellschaftlichen Vorurteile übernimmt und möglicherweise verstärkt besteht dennoch.\cite{silberg2019notes} Die Auswirkungen eines Bias im produktiven Einsatz äußern sich meist in Diskriminierung und Benachteiligung. 
        Um zu zeigen welche Folgen eine Verzerrung in einer \ac*{KI} haben kann, werden diese an drei berühmten Fällen von Diskriminierung und Benachteiligung aus der realen Welt demonstriert.
        \\
        Ein sehr populär gewordener Fall von Diskriminierung fand im Jahr 2015 im Zusammenhang mit der \ac*{KI} für Bilderkennung und automatisierte Kategorisierung von Google statt. Aufgabe der Software ist es, Bilder zu analysieren und dazu passende Beschriftungen zu erzeugen. Der Skandal der sich hier ereignete war, dass ein Foto von zwei Schwarzen Menschen mit der Beschriftung \glqq{}Gorillas\grqq{} versehen wurde.\cite{Cremers2019}\cite{IncidentDatabase2015_16}
        \begin{figure}[h]
            \centering
            \includegraphics[width = 11.5cm]{Bilder/Gorillas.png}
            \caption{Erkennung von Schwarzen als Gorillas \cite{IncidentDatabase2015_16}}
            \label{fig:HumanGorillas}
        \end{figure} \\
        In dem Bildausschnitt \ref*{fig:HumanGorillas} lässt sich dabei klar erkennen, dass die Kategorien der anderen Fotos treffend waren. Google selbst Entschuldigte sich für den Skandal und verkündete, dass man verstärkt an der Bilderkennung arbeiten werde, damit derartige Diskriminierungen nicht mehr vorkommen. Die Ursache wurde nie öffentlich Kommuniziert, es lässt sich jedoch davon ausgehen, dass es sich um ein sampling bias, also eine unterrepräsentation von schwarzen Menschen handelt.\cite{IncidentDatabase2015_16}
        \\
        Ein weiteres Beispiel, dass insbesondere die Entstehung eines Bias während des produktiven Betrieb veranschaulicht, war der Microsoft Chat bot Tay. Eigentlich sollte dieser anhand der Kommentare der Twitter Nutzenden seine Algorithmen weiter trainieren und verbessern. Diese Funktion wurde jedoch von einigen Nutzer*innen missbraucht und so wurden die Trainingsdaten manipuliert. Es wurden rassistische, diskriminierende und meinungsbetonten Kommentare veröffentlicht, die dazu geführt haben, dass das \ac*{ML} Modell sich dieses Verhalten selbst aneignet.\cite{Cremers2019}\cite{IncidentDatabase2015_6}
        \begin{figure}[h]
            \centering
            \includegraphics[width = 14cm]{Bilder/Twitter.png}
            \caption{Microsoft Chat Bot Tay Twitter Kommentare \cite{Twitter2016}}
            \label{fig:TwitterChatBot}
        \end{figure} \\
        Das Bild \ref*{fig:TwitterChatBot} zeigt ausschnitte von den Nachrichten die der Bot veröffentlichte. Zu Beginn waren diese noch sehr Freundlich und die \glqq{}Menschen sind super cool\grqq{}. Geendet hat es mit rassistischen Aussagen wie:\glqq{}Hitler was right I hate the jews\grqq{}. Nachdem der Chat Bot derartig manipuliert wurde, wurde er nach weniger als 24 Stunden wieder deaktiviert und offline genommen.\cite{IncidentDatabase2015_16} 
        \\
        Bei dem letzten Beispiel geht es um den Bewerbungsprozess bei Amazon. Dieses Beispiel bezieht sich auf eine Software die bereits 2014 entwickelt wurde und die Lebensläufe der Bewerber einstufen und Bewerten sollte. Nach einiger Zeit stellte sich heraus, dass die \ac*{KI} männliche Bewerber bevorzugt und Frauen unberechtigter weise schlechter bewertet. Die Ursache dafür liegt wiedermal in den Trainingsdaten des \ac*{ML} Modells. Es wurden die Daten der Bewerbungen der letzten Jahre genutzt. Da das Verhältnis von Männern und Frauen in der Tech Branche jedoch sehr unausgeglichen ist, hat das Modell den Trugschluss getroffen, dass männliche Bewerber bevorzugt werden würden. Auf diese Art und Weise entwickelte sich ein sampling bzw. measurement bias, der für eine Diskriminierung von Frauen sorgte. Letztendlich wurde die Software nach Versuchen, das Problem in den Griff zu bekommen, 2017 abgeschaltet.\cite{Cremers2019}\cite{IncidentDatabase2015_37}
        \\
        In allen Fällen ist die Verzerrung im produktiven Betrieb der \ac*{KI} entdeckt geworden. Daraus ergibt sich ein weiteres Problem. Nicht nur der Bias selbst ist ein Herausforderung, die es zu lösen gilt, sondern auch die Überprüfung von Modellen speziell hinsichtlich Verzerrungen. Denn \ac*{KI} hat inzwischen einen Stellenwert in der Gesellschaft, von dem aus nicht nur eine diskriminierende Nachricht oder ein rassistischer Kommentar die Folge einer Verzerrung sein kann. Es werden Entscheidungen durch \ac*{KI} getroffen, die einen starken Einfluss auf ein Individuum haben kann. Nur um einige Beispiele zu nennen werden mehr und mehr Entscheidungen im Gesundheitssektor oder der Justiz durch eine \ac*{KI} getroffen. Hier kann ein Bias verheerende Folgen haben, die nicht mit einer Entschuldigung korrigiert werden können.\cite{hagendorff2019maschinelles}Dabei ist die Form und die Ursache des Bias egal. Es muss grundsätzlich der Faktor Verzerrung, Bias und Vorurteile mehr berücksichtigt werden im Umgang mit \ac*{KI} Systemen.\cite{Drew2019}
    
    \subsection{Gegenma{\ss}nahmen}
    \label{subsubsec:gegenmassnahmen}
        Die Problematik der Diskriminierung stellt sowohl die Wissenschaft als auch die Wirtschaft vor Herausforderungen. Zur Lösung dieser Herausforderungen gibt es bereits eine Vielzahl an Forschungen, auch wenn die Thematik rund um Bias und Verzerrung einer \ac*{KI} erst ein sehr neuer Forschungsbereich ist.
        \\
        Entscheidend, um bewusst Gegenma{ss}nahmen gegen einen Bias einleiten zu können ist die Transparenz der \ac*{KI}. Die Transparenz ermöglicht es, Entscheidungen einer \ac*{KI} nachzuvollziehen und mögliche Verzerrungen zu identifizieren. Für die Erkennung und Eliminierung gibt es verschiedene Ansätze. Dazu gehören sowohl technische Maßnahmen, als auch organisatorische Maßnahmen die ergriffen werden können. 
        \begin{description}
            \item[Technische Maßnahmen] \hfill
            \begin{description}
                \item[Bereinigung der Trainingsdaten] \hfill \\
                Die Bereinigung der Trainingsdaten ist eine mögliche Maßnahme im Umgang mit einem erkannten Bias. Dazu werden die Einflussfaktoren, bei einer Regression beispielsweise die Korrelationskoeffizienten, genauer analysiert. Als Reaktion auf einen unzulässigen Einflussfaktor wird dieser aus der Entscheidungsfindung entfernt. So hat das für den Bias ausschlaggebende Attribut zukünftig keinen Einfluss auf die Ausgabe der \ac*{KI} bzw. des \ac*{ML} Algorithmus. Es ist zudem Egal um welche Art von Bias es sich in den Daten handelt. Durch die Entfernung werden zukünftige Bewertungen nicht mehr Verzerrt sein sowie die Daten selbst. Es wird für das zukünftige Training verhindert, dass aus den Daten eine Verzerrung erlernt werden kann, die den Algorithmus und die Ausgabe beeinflussen können.\cite{hagendorff2019maschinelles} Neben der Bereinigung des Bias verliert das Modell jedoch auch an Genauigkeit. Umgangssprachlich gesagt, wird dem Modell seine Entscheidungsgrundlage weggenommen und dieses muss die Neuen, meist tatsächlichen Zusammenhänge erst erlernen. So kann es dazu führen, dass das Modell ohne das Attribut nicht genug generalisiert und es zum underfitting kommt. Häufig kann dem entgegengesteuert werden, in dem mehr Trainingsdaten verwendet werden. In extremen Fällen kann es aber auch passieren, dass das Modell keine anderen Zusammenhänge auffinden kann und somit die \ac*{KI} in diesem Anwendungsfall vollkommen unbrauchbar wird.\cite{hagendorff2019maschinelles}\cite{Fabi2022}

                \item[Konzept der \glqq{}counterfactual fairness\grqq{}] \hfill \\
                \glqq{}Counterfactual fairness\grqq{} (\ac*{dt}.: kontrafaktische Fairness) wird genutzt um eine \ac*{KI} auf Verzerrungen bzw. auf Fairness zu überprüfen. Dabei werden in einem Datensatz Einträge künstlich verändert. Es wird beispielsweise nur das Geschlecht oder die Hautfarbe in einem Eintrag verändert und überprüft, wie die \ac*{KI} darauf reagiert. Wenn keine Zusammenhänge zwischen Entscheidung und der veränderten Variable bestehen, sollte die Ausgabe der \ac*{KI} identisch sein. Die Methode ermöglicht eine simple Überprüfung von Verzerrungen. Am besten eignet sich dieses Verfahren bei der Überprüfung von Gleichberechtigungen von Minderheiten, Geschlechtern oder Herkunft von Personen.\cite{hagendorff2019maschinelles}

                \item[Filtern der Trainingsdaten] \hfill \\
                Das Filtern der Trainingsdaten ist eine Möglichkeit sowohl Transparenz zu schaffen, aber ebenso die Entstehung eines Bias präventiv zu vermeiden. Dabei werden die Datensätze und die Attribute spezifisch für den Kontext und den Rahmen ausgewählt. Auf diese Art und Weise wird ein Datensatz bestehend aus realen Daten auf eine künstliche Weise erstellt. Bei der Auswahl ist das Ziel mögliche Ursachen für Bias au{\ss}envor zu lassen und so einen idealen Trainingsdatensatz zu schaffen.
                Man läuft jedoch Gefahr durch die individuelle Auswahl der Daten einen sampling Bias zu erzeugen. Denn bei einer Auswahl können zum einen die Vorurteile und die persönliche Wahrnehmung der Person eine Rolle spielen. Zum Anderen kann es aber auch zu einer Unterrepräsentation in den Daten kommen. Die Folge daraus wäre, dass der Datensatz nicht mehr repräsentativ ist und das Modell möglicherweise zu schlecht generalisiert und underfittet.\cite{Google}\cite{ntoutsi2020bias}

            \end{description}

            \item[Organisatorische Maßnahmen] \hfill
            \begin{description}
                \item [Regelmäßig Überprüfung] \hfill \\
                Für die Frühzeitige Erkennung eines Bias ist eine regelmäßige Überprüfung zwingend notwendig. Zu überprüfen sind dabei alle Ursachen von Bias. Dazu gehört die Auswahl und Erstellung der Trainingsdaten, die Bewerter der Trainingsdaten und der \ac{ML} Algorithmus. 
                Dabei kann das Auditieren durch die Entwickler selbst durchgeführt werden oder von au{\ss}enstehenden Dritten. Entscheidend ist die Regelmäßigkeit der Überprüfung. Sowohl während der Entwicklung muss die Entstehung eines Bias überwacht werden, als auch während dem produktiven Einsatz der \ac{KI}.\cite{Drew2019}\cite{hagendorff2019maschinelles}\cite{Dilmegani2020}\cite{Google}

                \item[Prinzip von \glqq{}fairness through unawareness\grqq{}] \hfill \\
                Bei dem Konzept von \glqq{}fairness through unawareness\grqq{} (\ac*{dt}.:Fairness durch Unwissenheit) werden die zur Verfügung stehenden Daten beschnitten. Es werden sensible Daten aus dem Entscheidungsprozess entfernt. So werden mögliche Vorurteile durch Bewertende oder den Algorithmus minimiert, da die Verzerrte Variable nicht in den Daten enthalten ist. Problematisch an diesem Vorgehen ist, dass durch die Eliminierung der Attribute eine Unwissenheit entsteht und somit unter Umständen Zusammenhänge nicht mehr vorhanden sind. Es kann zu einer unterräpresentation führen und tatsächliche Zusammenhänge in Daten eliminieren. Die Vorgehensweise garantiert somit nicht, dass nach der Entfernung faire Entscheidungen getroffen werden.\cite{hagendorff2019maschinelles}

            \end{description}
            
        \end{description}
        Trotz dieser Maßnahmen wird es immer Verzerrungen und Vorurteile in Daten geben. Die Ansätze sind je nach Kontext unterschiedlich Effektiv und eignen sich teils besser oder schlechter. Mit den Konzepten bekommt man die Möglichkeit mögliche Verzerrungen zu minimieren oder gar ganz zu eliminieren. Wichtiger als das ist es aber über deren Existenz aufzuklären. Denn es ist in der Verantwortung der Entwickler sowohl in der Entwicklung als auch bei dem Betrieb die \ac*{KI} regelmäßig auf Verzerrungen zu überprüfen.\cite{Drew2019} Nur so kann letztendlich eine gerechte, diskriminierungsfreie vertrauenswürdige \ac*{KI} geschaffen werden, die nachvollziehbare Entscheidungen trifft.\cite{HEGKI2019}\cite{HEGKI2019Definition}\cite{Fabi2020}
        
    \newpage
\end{onehalfspace}